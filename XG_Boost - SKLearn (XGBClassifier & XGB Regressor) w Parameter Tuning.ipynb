{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,recall_score,precision_score,f1_score,r2_score,explained_variance_score\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset + Examining the dataset for intuition\n",
    "dataset = pd.read_csv('Sample.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataset - identify missing, incorrect, redundant data in order to proceed with the pre-processing phase\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Missing Data\n",
    "print(dataset.isnull().any())\n",
    "\n",
    "# For numerical data\n",
    "dataset['header'].fillna(dataset['header'].mean(), inplace = True) #replace missing data with mean/median etc.\n",
    "# For categorical data\n",
    "dataset['header'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns which are redundant\n",
    "dataset = dataset.drop(['header1', 'header2', 'header3'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset after pre-processing\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = dataset.iloc[:, 3:13].values #to take all rows, and required columns\n",
    "y = dataset.iloc[:, 13].values #to take all rows, and required columns\n",
    "\n",
    "print(pd.DataFrame(X))\n",
    "print(pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data (i.e. Creating dummy variables)\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "ct_1 = ColumnTransformer(\n",
    "        [('one_hot_encoder', OneHotEncoder(), [1])],\n",
    "        remainder = 'passthrough')\n",
    "X = np.array(ct_1.fit_transform(X), dtype = np.float)\n",
    "\n",
    "print(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 1 variable to avoid dummy variable trap if needed\n",
    "X = np.delete(X,[1], axis = 1)\n",
    "\n",
    "print(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is balanced\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CLASSIFYING QUESTION\n",
    "\n",
    "# Conduct GridSearch, to find the optimal hyper-parameters\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', #'binary:logistic' for 2 classes, 'multi:softprob' for > 2 classes\n",
    "                          tree_method='exact', \n",
    "                          early_stopping_rounds = 50)\n",
    "\n",
    "parameters = {'max_depth': [2, 4, 6],\n",
    "              'learning_rate': [0.01, 0.03, 0.05, 0.07], #so called `eta` value\n",
    "              'n_estimators': [100, 500], #higher number of trees if insufficient data and vice versa\n",
    "              'gamma': [0, 0.2],\n",
    "              'min_child_weight': [1, 4, 6],\n",
    "              'subsample': [0.8], #randomly sample before growing tree, prevents over-fitting\n",
    "              'colsample_bytree': [0.8], #randomly sample before growing tree, prevents over-fitting\n",
    "              #'seed': [10]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=-1, \n",
    "                   cv=10,\n",
    "                   scoring='accuracy',\n",
    "                   verbose=3, \n",
    "                   refit=True)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "'''\n",
    "# FOR REGRESSION QUESTION\n",
    "\n",
    "# Conduct GridSearch, to find the optimal hyper-parameters\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', \n",
    "                          tree_method='exact', \n",
    "                          early_stopping_rounds = 50)\n",
    "                          \n",
    "parameters = {'max_depth': [2, 4, 6],\n",
    "              'learning_rate': [0.01, 0.02, 0.3, 0.4], #so called `eta` value\n",
    "              'n_estimators': [100, 500], #higher number of trees if insufficient data and vice versa\n",
    "              'gamma': [0, 0.2],\n",
    "              'min_child_weight': [1, 4, 6],\n",
    "              'subsample': [0.8], #randomly sample before growing tree, prevents over-fitting\n",
    "              'colsample_bytree': [0.8], #randomly sample before growing tree, prevents over-fitting\n",
    "              #'seed': [10]\n",
    "             }\n",
    "             \n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=-1, \n",
    "                   cv=10,\n",
    "                   verbose=3, refit=True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model report:\n",
    "print (\"Model Report:\")\n",
    "print(\"Best: Accuracy of %f using %s\" % (clf.best_score_, clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation metrics - For Classification\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, dtype = 'int64')\n",
    "\n",
    "sn.set(font_scale=1)#for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt='g')# font size\n",
    "\n",
    "'''\n",
    "Model Evaluation metrics - For Regression\n",
    "print('Adj R-squared : ' + str(r2_score(y_test,y_pred)))\n",
    "print('Variance: ' + str(explained_variance_score(y_test,y_pred)))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the final set\n",
    "z_test = pd.read_csv('Churn_Modelling.csv')\n",
    "z_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into numpy array\n",
    "z_test = z_test.iloc[:, :].values\n",
    "print (pd.DataFrame(z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the final set\n",
    "z_pred = clf.predict(z_test)\n",
    "\n",
    "# Review predictions of final set\n",
    "print(pd.DataFrame(z_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required to add to z_test file\n",
    "predictions = np.concatenate([(z_test, z_pred)]).T\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns=[\"header1\", \"header2\"]).to_csv('prediction.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all as predictions\n",
    "\n",
    "predictions = pd.DataFrame(y_pred, columns=[\"header1\"]).to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
